{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbc5d4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x174441810>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "import time\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import zstandard as zstd\n",
    "import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from torchviz import make_dot\n",
    "from torchinfo import summary\n",
    "import random\n",
    "import shutil\n",
    "import csv\n",
    "import requests\n",
    "import scipy.stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1efb49a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: r75abf0a6t.tiff\n",
      "Downloaded: r1301676dt.tiff\n",
      "Downloaded: r0cea5432t.tiff\n",
      "Downloaded: raecffc3et.tiff\n",
      "Downloaded: re8712078t.tiff\n",
      "Downloaded: r11e134c7t.tiff\n",
      "Downloaded: r45f0bdf7t.tiff\n",
      "Downloaded: r647d6726t.tiff\n",
      "Downloaded: r28360ddbt.tiff\n",
      "Downloaded: r1a9de6e9t.tiff\n",
      "Downloaded: r5a671b67t.tiff\n",
      "Downloaded: rdb736502t.tiff\n",
      "Downloaded: r56d61c92t.tiff\n",
      "Downloaded: r66327327t.tiff\n",
      "Downloaded: r64b46b1et.tiff\n",
      "Downloaded: rca0a29eft.tiff\n",
      "Downloaded: r4091b09ct.tiff\n",
      "Downloaded: raaebdc82t.tiff\n",
      "Downloaded: r8500c118t.tiff\n",
      "Downloaded: rd169bc43t.tiff\n",
      "Downloaded: r069e346et.tiff\n",
      "Downloaded: r3bacc1d2t.tiff\n",
      "Downloaded: r39436d87t.tiff\n",
      "Downloaded: r95b08e31t.tiff\n",
      "Downloaded: r885079b8t.tiff\n",
      "Downloaded: rad5b5878t.tiff\n",
      "Downloaded: r4359075bt.tiff\n",
      "Downloaded: r65b20842t.tiff\n",
      "Downloaded: r8abcf61dt.tiff\n",
      "Downloaded: r0c5e9567t.tiff\n",
      "Downloaded: r8578eafdt.tiff\n",
      "Downloaded: r179ec4e8t.tiff\n",
      "Downloaded: rc5c4bbd3t.tiff\n",
      "Downloaded: r209f6d4at.tiff\n",
      "Downloaded: r9c8bd24et.tiff\n",
      "Downloaded: rbfb29a75t.tiff\n",
      "Downloaded: r5c782fcft.tiff\n",
      "Downloaded: re5569a15t.tiff\n",
      "Downloaded: r4e051620t.tiff\n",
      "Downloaded: r3afdd207t.tiff\n",
      "Downloaded: r437a39bft.tiff\n",
      "Downloaded: r9801f080t.tiff\n",
      "Downloaded: r453acda3t.tiff\n",
      "Downloaded: r7393d71et.tiff\n",
      "Downloaded: rca369735t.tiff\n",
      "Downloaded: r5db46b31t.tiff\n",
      "Downloaded: r89c44339t.tiff\n",
      "Downloaded: r9158953ft.tiff\n",
      "Downloaded: r6d133f14t.tiff\n",
      "Downloaded: rb70fb952t.tiff\n",
      "Downloaded: r17228bbdt.tiff\n",
      "Downloaded: r2a59ecf5t.tiff\n",
      "Downloaded: raf7ad161t.tiff\n",
      "Downloaded: rc1e9d36bt.tiff\n",
      "Downloaded: r3c5df3f6t.tiff\n",
      "Downloaded: r505d386ft.tiff\n",
      "Downloaded: r19fe3107t.tiff\n",
      "Downloaded: rae77a6f5t.tiff\n",
      "Downloaded: ra09acb48t.tiff\n",
      "Downloaded: recdce114t.tiff\n",
      "Downloaded: rd3233a58t.tiff\n",
      "Downloaded: rbc105d4at.tiff\n",
      "Downloaded: ra0a0301dt.tiff\n",
      "Downloaded: r066ef803t.tiff\n",
      "Downloaded: r3db9bd34t.tiff\n",
      "Downloaded: r7d81a340t.tiff\n",
      "Downloaded: r6f056feft.tiff\n",
      "Downloaded: r3ab1cb3at.tiff\n",
      "Downloaded: r08873591t.tiff\n",
      "Downloaded: r17d11b3at.tiff\n",
      "Downloaded: r85ad6c43t.tiff\n",
      "Downloaded: r21ea8f3dt.tiff\n",
      "Downloaded: r6e541790t.tiff\n",
      "Downloaded: r94fc99c1t.tiff\n",
      "Downloaded: rf9a6bad6t.tiff\n",
      "Downloaded: r1c146589t.tiff\n",
      "Downloaded: ree7dd2f3t.tiff\n",
      "Downloaded: r110cc74ct.tiff\n",
      "Downloaded: re6d3126et.tiff\n",
      "Downloaded: r3602d116t.tiff\n",
      "Downloaded: rda3a8fect.tiff\n",
      "Downloaded: refad7420t.tiff\n",
      "Downloaded: rbcc1d560t.tiff\n",
      "Downloaded: r9ab99c3ct.tiff\n",
      "Downloaded: raa47365at.tiff\n",
      "Downloaded: rc69ae34dt.tiff\n",
      "Downloaded: r3767e64ct.tiff\n",
      "Downloaded: r381a88d3t.tiff\n",
      "Downloaded: r645823eft.tiff\n",
      "Downloaded: rd007cdaat.tiff\n",
      "Downloaded: rc9316ecct.tiff\n",
      "Downloaded: raf8b0136t.tiff\n",
      "Downloaded: r1618da9dt.tiff\n",
      "Downloaded: r94f061eft.tiff\n",
      "Downloaded: ra6e53ca7t.tiff\n",
      "Downloaded: rb107bbb5t.tiff\n",
      "Downloaded: r5ac60ddet.tiff\n",
      "Downloaded: ra0664463t.tiff\n",
      "Downloaded: r2dba464at.tiff\n",
      "Downloaded: ra27f89b4t.tiff\n",
      "Downloaded: r7defe377t.tiff\n",
      "Downloaded: r094cf3c6t.tiff\n",
      "Downloaded: r83c46cbbt.tiff\n",
      "Downloaded: r27d60243t.tiff\n",
      "Downloaded: rdd91980et.tiff\n",
      "Downloaded: ra605fb87t.tiff\n",
      "Downloaded: r4b34cdeat.tiff\n",
      "Downloaded: rf1a9430bt.tiff\n",
      "Downloaded: rd5608735t.tiff\n",
      "Downloaded: rc64da437t.tiff\n",
      "Downloaded: rf5c51ae1t.tiff\n",
      "Downloaded: ra6a8c6edt.tiff\n",
      "Downloaded: rae3a362bt.tiff\n",
      "Downloaded: r23beab04t.tiff\n",
      "Downloaded: r55a12137t.tiff\n",
      "Downloaded: r1297fd0bt.tiff\n",
      "Downloaded: r6e198a14t.tiff\n",
      "Downloaded: r7b276a02t.tiff\n",
      "Downloaded: r8503e867t.tiff\n",
      "Downloaded: re5098abat.tiff\n",
      "Downloaded: r32457827t.tiff\n",
      "Downloaded: r8b49780ct.tiff\n",
      "Downloaded: r68fbc29ct.tiff\n",
      "Downloaded: rc297369dt.tiff\n",
      "Downloaded: raabe7157t.tiff\n",
      "Downloaded: r787cb16dt.tiff\n",
      "Downloaded: r67cd393at.tiff\n",
      "Downloaded: rec44174et.tiff\n",
      "Downloaded: r806556eft.tiff\n",
      "Downloaded: r2380441ct.tiff\n",
      "Downloaded: rc9b60c2ft.tiff\n",
      "Downloaded: r2a94cf55t.tiff\n",
      "Downloaded: rba5d08b5t.tiff\n",
      "Downloaded: re55162b9t.tiff\n",
      "Downloaded: r698c77d5t.tiff\n",
      "Downloaded: r91bcee97t.tiff\n",
      "Downloaded: r6fcd83d4t.tiff\n",
      "Downloaded: rdc73b9a5t.tiff\n",
      "Downloaded: r2507b345t.tiff\n",
      "Downloaded: r3725d9ddt.tiff\n",
      "Downloaded: r6555b778t.tiff\n",
      "Downloaded: r3e62a376t.tiff\n",
      "Downloaded: r87a3d7f5t.tiff\n",
      "Downloaded: r000da54ft.tiff\n",
      "Downloaded: r9eafe8fft.tiff\n",
      "Downloaded: r91447b32t.tiff\n",
      "Downloaded: rc47a7997t.tiff\n",
      "Downloaded: r973e22dct.tiff\n",
      "Downloaded: r6e9a16c9t.tiff\n",
      "Downloaded: r452a036bt.tiff\n",
      "Downloaded: re9cfc8b3t.tiff\n",
      "Downloaded: r772c40d7t.tiff\n",
      "Downloaded: re36d74cct.tiff\n",
      "Downloaded: r831140dct.tiff\n",
      "Downloaded: rd8d2128ct.tiff\n",
      "Downloaded: rc3e4affft.tiff\n",
      "Downloaded: rbf8b293dt.tiff\n",
      "Downloaded: r9cfeabe5t.tiff\n",
      "Downloaded: rd552cb46t.tiff\n",
      "Downloaded: r7783c219t.tiff\n",
      "Downloaded: r4fcc8457t.tiff\n",
      "Downloaded: r62572141t.tiff\n",
      "Downloaded: ra768d61ct.tiff\n",
      "Downloaded: r2da9db0ct.tiff\n",
      "Downloaded: r5e94913at.tiff\n",
      "Downloaded: r23f2406at.tiff\n",
      "Downloaded: r5476adbdt.tiff\n",
      "Downloaded: rcf4693b8t.tiff\n",
      "Downloaded: r8dc8b8a4t.tiff\n",
      "Downloaded: r32cd478dt.tiff\n",
      "Downloaded: re16e9844t.tiff\n",
      "Downloaded: r062cab38t.tiff\n",
      "Downloaded: re869b2b0t.tiff\n",
      "Downloaded: r5543795dt.tiff\n",
      "Downloaded: r8f496059t.tiff\n",
      "Downloaded: rd40ede61t.tiff\n",
      "Downloaded: ra004d893t.tiff\n",
      "Downloaded: r81dee391t.tiff\n",
      "Downloaded: r20ced612t.tiff\n",
      "Downloaded: r7ed42d24t.tiff\n",
      "Downloaded: r542b255et.tiff\n",
      "Downloaded: r8315ada4t.tiff\n",
      "Downloaded: rf35f579ft.tiff\n",
      "Downloaded: r662c01bdt.tiff\n",
      "Downloaded: r6efdd2cft.tiff\n",
      "Downloaded: r3276d32ct.tiff\n",
      "Downloaded: r5c95b318t.tiff\n",
      "Downloaded: rf45380det.tiff\n",
      "Downloaded: rdde9a595t.tiff\n",
      "Downloaded: rd0fdedd3t.tiff\n",
      "Downloaded: r522b6f0dt.tiff\n",
      "Downloaded: ree0c86dbt.tiff\n",
      "Downloaded: r1ffa9ee3t.tiff\n",
      "Downloaded: r1ab0bb20t.tiff\n",
      "Downloaded: r9dcf33e7t.tiff\n",
      "Downloaded: r3ae47fdat.tiff\n",
      "Downloaded: r42835b48t.tiff\n",
      "Downloaded: raf9233fct.tiff\n",
      "Downloaded: r1c6377bat.tiff\n",
      "Downloaded: rdcca1488t.tiff\n",
      "Downloaded: rd84c3237t.tiff\n",
      "Downloaded: r6a621922t.tiff\n",
      "Downloaded: r040b3002t.tiff\n",
      "Downloaded: r468fb546t.tiff\n",
      "Downloaded: r7fbb84c3t.tiff\n",
      "Downloaded: r027a692ft.tiff\n",
      "Downloaded: redad3c0at.tiff\n",
      "Downloaded: r21b7ba47t.tiff\n",
      "Downloaded: r3ac3042ft.tiff\n",
      "Downloaded: r60c520ddt.tiff\n",
      "Downloaded: r48797c85t.tiff\n",
      "Downloaded: rcdf7ebb6t.tiff\n",
      "Downloaded: rac3f64aft.tiff\n",
      "Downloaded: rbbdfd7a1t.tiff\n",
      "Downloaded: r498397f5t.tiff\n",
      "Downloaded: r5e5f6baat.tiff\n",
      "Downloaded: r83205374t.tiff\n",
      "Downloaded: rd271dad6t.tiff\n",
      "Downloaded: rd23122f3t.tiff\n",
      "Downloaded: r3e6473ect.tiff\n",
      "Downloaded: r68116690t.tiff\n",
      "Downloaded: r1eff17eft.tiff\n",
      "Downloaded: r90bb7d2dt.tiff\n",
      "Downloaded: ra95d9fc7t.tiff\n",
      "Downloaded: r54f1d1bbt.tiff\n",
      "Downloaded: r535cbd86t.tiff\n",
      "Downloaded: r029afdb0t.tiff\n",
      "Downloaded: rc514974ct.tiff\n",
      "Downloaded: r781dc7a8t.tiff\n",
      "Downloaded: rd6d327d5t.tiff\n",
      "Downloaded: r7f41547dt.tiff\n",
      "Downloaded: r773ce522t.tiff\n",
      "Downloaded: rb3e65c98t.tiff\n",
      "Downloaded: rf35a9426t.tiff\n",
      "Downloaded: rde37348dt.tiff\n",
      "Downloaded: r8f53c94dt.tiff\n",
      "Downloaded: r00816405t.tiff\n",
      "Downloaded: re097e59bt.tiff\n",
      "Downloaded: ra7cc8d18t.tiff\n",
      "Downloaded: r46819a42t.tiff\n",
      "Downloaded: refe77649t.tiff\n",
      "Downloaded: r34f7e663t.tiff\n",
      "Downloaded: r7b8b40aat.tiff\n",
      "Downloaded: r08d2b4dat.tiff\n",
      "Downloaded: rb41f6e92t.tiff\n",
      "Downloaded: r8bdd8881t.tiff\n",
      "Downloaded: r9e8e57e4t.tiff\n",
      "Downloaded: r4f24f947t.tiff\n",
      "Downloaded: r50a7b6bbt.tiff\n",
      "Downloaded: re128c862t.tiff\n",
      "Downloaded: race5a2bct.tiff\n",
      "Downloaded: r4b62d2d1t.tiff\n",
      "Downloaded: r78ff6ed1t.tiff\n",
      "Downloaded: r224b8509t.tiff\n",
      "Downloaded: r36f7a056t.tiff\n",
      "Downloaded: rf5ccbe95t.tiff\n",
      "Downloaded: r8de0558ct.tiff\n",
      "Downloaded: rc02b5fb0t.tiff\n",
      "Downloaded: recb71167t.tiff\n",
      "Downloaded: r789963d1t.tiff\n",
      "Downloaded: red5d106ct.tiff\n",
      "Downloaded: r77ca96d3t.tiff\n",
      "Downloaded: ree427299t.tiff\n",
      "Downloaded: ra5d3d636t.tiff\n",
      "Downloaded: rba8f0e6et.tiff\n",
      "Downloaded: r360722f4t.tiff\n",
      "Downloaded: r008a2095t.tiff\n",
      "Downloaded: rc154ddc1t.tiff\n",
      "Downloaded: r188eb234t.tiff\n",
      "Downloaded: r355a1c3dt.tiff\n",
      "Downloaded: rfed1f3cct.tiff\n"
     ]
    }
   ],
   "source": [
    "def download_images(csv_file):\n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        reader.__next__()  # Skip header\n",
    "        for row in reader:\n",
    "            tiff_link = row[2]\n",
    "            tiff_name = row[0] + '.tiff'\n",
    "            tiff_path = os.path.join('images', tiff_name)\n",
    "            if not os.path.exists(tiff_path):\n",
    "                response = requests.get(tiff_link)\n",
    "                with open(tiff_path, 'wb') as img_file:\n",
    "                    img_file.write(response.content)\n",
    "                print(f'Downloaded: {tiff_name}')\n",
    "\n",
    "os.makedirs('images', exist_ok=True)\n",
    "download_images('RAISE_8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bf6bc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive streaming dataset analysis...\n",
      "Starting comprehensive analysis of images in 'images'...\n",
      "\n",
      "--- Summary Statistics ---\n",
      "Total images processed: 271\n",
      "Average Dimensions: 4312 x 3479 pixels\n",
      "Average File Size: 23249.44 KB\n",
      "Average Aspect Ratio: 1.30\n",
      "Average Unique Colors: 903601\n",
      "Average Texture (Std Dev): 55.90\n",
      "Average Sharpness (Laplacian Variance): 377.93\n",
      "\n",
      "All plots and statistics for 'images' saved to 'descriptive_stats/images/'\n",
      "\n",
      "Analysis complete.\n"
     ]
    }
   ],
   "source": [
    "def get_descriptive_statistics(directory, output_dir_base):\n",
    "    \"\"\"\n",
    "    Analyzes an image dataset using a streaming approach to conserve memory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the image directory.\n",
    "        output_dir_base (str): Base directory for output.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Error: Directory '{directory}' not found.\")\n",
    "        return\n",
    "\n",
    "    output_dir = os.path.join(output_dir_base, os.path.basename(directory))\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    image_files = [f for f in Path(directory).glob('*') if f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff']]\n",
    "    if not image_files:\n",
    "        print(f\"No images found in '{directory}'. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Streaming variables for calculations\n",
    "    num_images = 0\n",
    "    file_sizes = []\n",
    "    \n",
    "    widths = []\n",
    "    heights = []\n",
    "    aspect_ratios = []\n",
    "\n",
    "    # New metrics for each image\n",
    "    avg_stds = []\n",
    "    laplacian_variances = []\n",
    "    image_entropies = []\n",
    "    unique_colors_list = []\n",
    "    \n",
    "    # Pre-allocated histogram bins\n",
    "    intensity_bins = np.zeros(256, dtype=int)\n",
    "\n",
    "    print(f\"Starting comprehensive analysis of images in '{directory}'...\")\n",
    "    for img_path in image_files:\n",
    "        try:\n",
    "            num_images += 1\n",
    "            file_size_kb = os.path.getsize(img_path) / 1024.0\n",
    "            file_sizes.append(file_size_kb)\n",
    "\n",
    "            with Image.open(img_path) as img:\n",
    "                img_width, img_height = img.size\n",
    "                widths.append(img_width)\n",
    "                heights.append(img_height)\n",
    "                aspect_ratios.append(img_width / img_height)\n",
    "                \n",
    "                # Convert to RGB to handle grayscale images consistently for unique colors\n",
    "                rgb_img = img.convert('RGB')\n",
    "                unique_colors_list.append(len(rgb_img.getcolors(maxcolors=rgb_img.size[0]*rgb_img.size[1])))\n",
    "\n",
    "                # Convert to grayscale for consistent intensity, texture, and sharpness analysis\n",
    "                gray_img_array = np.array(img.convert('L'))\n",
    "                \n",
    "                # Pixel Intensity Stats\n",
    "                pixels = gray_img_array.flatten()\n",
    "                \n",
    "                # Add to histogram bins\n",
    "                counts = np.bincount(pixels, minlength=256)\n",
    "                intensity_bins += counts\n",
    "                \n",
    "                # Texture & Complexity\n",
    "                avg_stds.append(np.std(pixels))\n",
    "                image_entropies.append(scipy.stats.entropy(counts / counts.sum()))\n",
    "                \n",
    "                # Sharpness (Laplacian Variance)\n",
    "                laplacian = cv2.Laplacian(gray_img_array, cv2.CV_64F)\n",
    "                laplacian_variances.append(laplacian.var())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not process '{img_path}'. Reason: {e}\")\n",
    "            continue\n",
    "\n",
    "    if num_images == 0:\n",
    "        print(f\"No valid images processed in '{directory}'.\")\n",
    "        return\n",
    "    \n",
    "    # Final Calculations and Summary\n",
    "    print(\"\\n--- Summary Statistics ---\")\n",
    "    print(f\"Total images processed: {num_images}\")\n",
    "    print(f\"Average Dimensions: {np.mean(widths):.0f} x {np.mean(heights):.0f} pixels\")\n",
    "    print(f\"Average File Size: {np.mean(file_sizes):.2f} KB\")\n",
    "    print(f\"Average Aspect Ratio: {np.mean(aspect_ratios):.2f}\")\n",
    "    print(f\"Average Unique Colors: {np.mean(unique_colors_list):.0f}\")\n",
    "    print(f\"Average Texture (Std Dev): {np.mean(avg_stds):.2f}\")\n",
    "    print(f\"Average Sharpness (Laplacian Variance): {np.mean(laplacian_variances):.2f}\")\n",
    "    \n",
    "    # Plotting\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "    # Histogram for Aspect Ratio\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(aspect_ratios, bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    plt.title(f'Aspect Ratio Distribution for {os.path.basename(directory)}')\n",
    "    plt.xlabel('Aspect Ratio (Width / Height)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.savefig(os.path.join(output_dir, 'aspect_ratio_distribution.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Histogram for Pixel Intensity Standard Deviation (Texture)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(avg_stds, bins=50, edgecolor='black', alpha=0.7, color='purple')\n",
    "    plt.title(f'Pixel Intensity Standard Deviation (Texture) Distribution for {os.path.basename(directory)}')\n",
    "    plt.xlabel('Pixel Intensity Std Dev')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.savefig(os.path.join(output_dir, 'pixel_std_dev_distribution.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Histogram for Laplacian Variance (Sharpness)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(laplacian_variances, bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "    plt.title(f'Laplacian Variance (Sharpness) Distribution for {os.path.basename(directory)}')\n",
    "    plt.xlabel('Laplacian Variance')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.savefig(os.path.join(output_dir, 'laplacian_variance_distribution.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Scatter Plot: File Size vs. Sharpness\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(laplacian_variances, file_sizes, alpha=0.6, color='seagreen')\n",
    "    plt.title(f'File Size vs. Sharpness for {os.path.basename(directory)}')\n",
    "    plt.xlabel('Laplacian Variance (Sharpness)')\n",
    "    plt.ylabel('File Size (KB)')\n",
    "    plt.savefig(os.path.join(output_dir, 'file_size_vs_sharpness.png'))\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(256), intensity_bins, color='lightgreen', width=1.0, edgecolor='black', linewidth=0.5)\n",
    "    plt.title(f'Pixel Intensity Distribution for {os.path.basename(directory)}')\n",
    "    plt.xlabel('Pixel Intensity (0-255)')\n",
    "    plt.ylabel('Total Pixel Count')\n",
    "    plt.savefig(os.path.join(output_dir, 'pixel_intensity_distribution.png'))\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "    # Scatter Plot: File Size vs. Texture\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(avg_stds, file_sizes, alpha=0.6, color='darkorange')\n",
    "    plt.title(f'File Size vs. Texture for {os.path.basename(directory)}')\n",
    "    plt.xlabel('Pixel Intensity Std Dev (Texture)')\n",
    "    plt.ylabel('File Size (KB)')\n",
    "    plt.savefig(os.path.join(output_dir, 'file_size_vs_texture.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"\\nAll plots and statistics for '{directory}' saved to '{output_dir}/'\")\n",
    "\n",
    "\n",
    "images_dir = 'images'\n",
    "output_dir_base = 'descriptive_stats'\n",
    "\n",
    "print(\"Starting comprehensive streaming dataset analysis...\")\n",
    "\n",
    "get_descriptive_statistics(images_dir, output_dir_base)\n",
    "\n",
    "print(\"\\nAnalysis complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2627014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files found: 271\n",
      "Copying 217 files to TRAIN\n",
      "Copying 54 files to TEST\n",
      "Train-test split complete!\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(path_to_images_folder, test_size=0.2):\n",
    "    # Convert path to a Path object for easier handling\n",
    "    path_to_images = Path(path_to_images_folder)\n",
    "\n",
    "    # Check if the source directory exists\n",
    "    if not path_to_images.is_dir():\n",
    "        print(f\"Error: The specified folder does not exist at {path_to_images}\")\n",
    "        return\n",
    "\n",
    "    # Define the output directories\n",
    "    train_dir = Path('TRAIN')\n",
    "    test_dir = Path('TEST')\n",
    "\n",
    "    # Create the output directories if they don't exist\n",
    "    for directory in [train_dir, test_dir]:\n",
    "        directory.mkdir(exist_ok=True)\n",
    "\n",
    "    # Get a list of all image files in the source folder\n",
    "    # We'll use a simple approach, assuming image files have common extensions\n",
    "    all_files = [f for f in path_to_images.iterdir() if f.is_file() and f.suffix.lower() in ['.tiff']]\n",
    "\n",
    "    # Shuffle the list of files to ensure randomness\n",
    "    random.shuffle(all_files)\n",
    "\n",
    "    # Calculate the number of files for the test set\n",
    "    test_count = int(len(all_files) * test_size)\n",
    "    \n",
    "    # Split the files into test and train sets\n",
    "    test_files = all_files[:test_count]\n",
    "    train_files = all_files[test_count:]\n",
    "\n",
    "    print(f\"Total files found: {len(all_files)}\")\n",
    "    print(f\"Copying {len(train_files)} files to {train_dir}\")\n",
    "\n",
    "    for file in train_files:\n",
    "        shutil.copy(str(file), str(train_dir / file.name))\n",
    "\n",
    "    print(f\"Copying {len(test_files)} files to {test_dir}\")\n",
    "\n",
    "    for file in test_files:\n",
    "        shutil.copy(str(file), str(test_dir / file.name))\n",
    "        \n",
    "    print(\"Train-test split complete!\")\n",
    "\n",
    "train_test_split('images', test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93ff7099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating raw JPEG compression with quality=90...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 54/54 [00:13<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation complete. Results saved to 'raw_jpeg_results/'\n"
     ]
    }
   ],
   "source": [
    "def load_image(path, resize=None):\n",
    "    \"\"\"Load an image and convert to RGB.\"\"\"\n",
    "    try:\n",
    "        rgb = cv2.imread(str(path))\n",
    "        if rgb is None:\n",
    "            return None\n",
    "        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "        if resize is not None:\n",
    "            rgb = cv2.resize(rgb, resize, interpolation=cv2.INTER_AREA)\n",
    "        return rgb\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_metrics(original, reconstructed):\n",
    "    \"\"\"\n",
    "    Calculate compression metrics, handling cases where images are identical.\n",
    "    \"\"\"\n",
    "    original_gray = cv2.cvtColor(original, cv2.COLOR_RGB2GRAY)\n",
    "    reconstructed_gray = cv2.cvtColor(reconstructed, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Check if images are identical; if so, SSIM is 1.0 to avoid 'inf'\n",
    "    if np.all(original == reconstructed):\n",
    "        psnr_val = psnr(original, reconstructed, data_range=255)\n",
    "        ssim_val = 1.0  \n",
    "    else:\n",
    "        psnr_val = psnr(original, reconstructed, data_range=255)\n",
    "        ssim_val = ssim(original_gray, reconstructed_gray, data_range=255)\n",
    "    \n",
    "    return {'psnr': psnr_val, 'ssim': ssim_val}\n",
    "\n",
    "def evaluate_raw_jpeg_compression(\n",
    "    images_dir='TEST',\n",
    "    output_dir='raw_jpeg_results',\n",
    "    target_size=(256, 256),\n",
    "    num_examples=10,\n",
    "    jpeg_quality=90\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates raw JPEG compression performance on a dataset of images.\n",
    "\n",
    "    Args:\n",
    "        images_dir (str): Directory containing the images to evaluate.\n",
    "        output_dir (str): Directory to save the results and plots.\n",
    "        target_size (tuple): The size to resize images to (width, height).\n",
    "        num_examples (int): The number of example images to save.\n",
    "        jpeg_quality (int): The JPEG quality level (0-100).\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'examples'), exist_ok=True)\n",
    "    \n",
    "    image_files = [f for f in Path(images_dir).glob('*') if f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']]\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_dir}\")\n",
    "        return\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Evaluating raw JPEG compression with quality={jpeg_quality}...\")\n",
    "    \n",
    "    for img_path in tqdm(image_files, desc=\"Processing images\"):\n",
    "        original_img = load_image(img_path, resize=target_size)\n",
    "        if original_img is None:\n",
    "            continue\n",
    "\n",
    "        # Step 1: Compress with JPEG to a memory buffer\n",
    "        img_pil = Image.fromarray(original_img)\n",
    "        jpeg_buffer = io.BytesIO()\n",
    "        img_pil.save(jpeg_buffer, format='JPEG', quality=jpeg_quality)\n",
    "        jpeg_buffer.seek(0)\n",
    "        \n",
    "        # Get compressed size\n",
    "        jpeg_size = len(jpeg_buffer.getvalue())\n",
    "\n",
    "        # Step 2: Decompress from the buffer to get the reconstructed image\n",
    "        reconstructed_img = np.array(Image.open(jpeg_buffer))\n",
    "\n",
    "        # Step 3: Calculate metrics\n",
    "        metrics = calculate_metrics(original_img, reconstructed_img)\n",
    "        \n",
    "        original_bytes = target_size[0] * target_size[1] * 3\n",
    "        compression_ratio = original_bytes / jpeg_size\n",
    "        bits_per_pixel = (jpeg_size * 8) / (target_size[0] * target_size[1])\n",
    "        \n",
    "        results.append({\n",
    "            'image_name': img_path.name,\n",
    "            'psnr': metrics['psnr'],\n",
    "            'ssim': metrics['ssim'],\n",
    "            'compression_ratio': compression_ratio,\n",
    "            'bits_per_pixel': bits_per_pixel,\n",
    "            'original_size': original_bytes,\n",
    "            'compressed_size': jpeg_size\n",
    "        })\n",
    "\n",
    "    # --- Analysis and Visualization ---\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(output_dir, 'raw_jpeg_results.csv'), index=False)\n",
    "    \n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    sns.histplot(df.replace([np.inf, -np.inf], np.nan)['psnr'], bins=20, ax=axes[0])\n",
    "    axes[0].set_title('Raw JPEG PSNR Distribution')\n",
    "    sns.histplot(df['compression_ratio'], bins=20, ax=axes[1])\n",
    "    axes[1].set_title('Raw JPEG Compression Ratio Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'raw_jpeg_metrics.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Save a few example images\n",
    "    for i in range(min(num_examples, len(image_files))):\n",
    "        img_path = image_files[i]\n",
    "        original_img = load_image(img_path, resize=target_size)\n",
    "        \n",
    "        img_pil = Image.fromarray(original_img)\n",
    "        jpeg_buffer = io.BytesIO()\n",
    "        img_pil.save(jpeg_buffer, format='JPEG', quality=jpeg_quality)\n",
    "        jpeg_buffer.seek(0)\n",
    "        reconstructed_img = np.array(Image.open(jpeg_buffer))\n",
    "        \n",
    "        metrics = calculate_metrics(original_img, reconstructed_img)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        axes[0].imshow(original_img)\n",
    "        axes[0].set_title('Original')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(reconstructed_img)\n",
    "        axes[1].set_title(f'JPEG Reconstruction\\nPSNR: {metrics[\"psnr\"]:.2f} dB, SSIM: {metrics[\"ssim\"]:.4f}')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'examples', f'jpeg_example_{i:02d}.png'), dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"\\nEvaluation complete. Results saved to '{output_dir}/'\")\n",
    "\n",
    "evaluate_raw_jpeg_compression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7517abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Standalone Autoencoder Experiment\n",
      "==================================================\n",
      "Using device: mps\n",
      "Found 200 training images and 50 test images.\n",
      "\n",
      "==================================================\n",
      "PHASE 1: LOADING TRAINING IMAGES\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading training images: 100%|██████████| 200/200 [00:50<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 200 training images\n",
      "\n",
      "==================================================\n",
      "PHASE 2: TRAINING AUTOENCODER\n",
      "==================================================\n",
      "Autoencoder initialized with 18,222,915 parameters\n",
      "Training on 200 images\n",
      "Epoch [5/100], Loss: 0.048520\n",
      "Epoch [10/100], Loss: 0.036996\n",
      "Epoch [15/100], Loss: 0.032570\n",
      "Epoch [20/100], Loss: 0.029977\n",
      "Epoch [25/100], Loss: 0.025756\n",
      "Epoch [30/100], Loss: 0.021420\n",
      "Epoch [35/100], Loss: 0.018286\n",
      "Epoch [40/100], Loss: 0.016204\n",
      "Epoch [45/100], Loss: 0.014666\n",
      "Epoch [50/100], Loss: 0.012181\n",
      "Epoch [55/100], Loss: 0.011980\n",
      "Epoch [60/100], Loss: 0.010346\n",
      "Epoch [65/100], Loss: 0.010331\n",
      "Epoch [70/100], Loss: 0.009119\n",
      "Epoch [75/100], Loss: 0.008302\n",
      "Epoch [80/100], Loss: 0.007539\n",
      "Epoch [85/100], Loss: 0.007632\n",
      "Epoch [90/100], Loss: 0.008419\n",
      "Epoch [95/100], Loss: 0.006858\n",
      "Epoch [100/100], Loss: 0.006652\n",
      "Training completed. Best loss: 0.006652\n",
      "Autoencoder training completed!\n",
      "\n",
      "==================================================\n",
      "PHASE 3: TESTING ON TEST IMAGES\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test images: 100%|██████████| 50/50 [00:14<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PHASE 4: ANALYZING RESULTS\n",
      "==================================================\n",
      "Results saved to autoencoder_results/autoencoder_results.csv\n",
      "\n",
      "Experiment completed successfully!\n",
      "Results saved to autoencoder_results/\n",
      "Model weights saved to autoencoder_results/models/\n",
      "Example images saved to autoencoder_results/examples/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Dataset for training the autoencoder on images.\"\"\"\n",
    "    def __init__(self, images):\n",
    "        self.images = images\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        image_tensor = torch.from_numpy(image).float() / 255.0\n",
    "        image_tensor = image_tensor.permute(2, 0, 1)\n",
    "        return image_tensor\n",
    "\n",
    "class SimpleAutoencoder(nn.Module):\n",
    "    def __init__(self, input_channels=3, input_size=256, latent_dim=128):\n",
    "        super(SimpleAutoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, 4, stride=2, padding=1), # 128x128\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 64x64\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 32x32\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1), # 16x16\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # Bottleneck to compress the latent space\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.encoder_output_size = 16 * 16 * 256\n",
    "        self.bottleneck = nn.Linear(self.encoder_output_size, self.latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_linear = nn.Linear(self.latent_dim, self.encoder_output_size)\n",
    "        self.unflatten = nn.Unflatten(1, (256, 16, 16))\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1), # 32x32\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),  # 64x64\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),   # 128x128\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, input_channels, 4, stride=2, padding=1), # 256x256\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.bottleneck(x)\n",
    "    \n",
    "    def decode(self, x):\n",
    "        x = self.decoder_linear(x)\n",
    "        x = self.unflatten(x)\n",
    "        return self.decoder(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        latent = self.encode(x)\n",
    "        return self.decode(latent)\n",
    "\n",
    "class SimpleAutoencoderExperiment:\n",
    "    \"\"\"Main class for conducting the simple autoencoder compression experiment.\"\"\"\n",
    "    \n",
    "    def __init__(self, train_dir='TRAIN', test_dir='TEST', output_dir='autoencoder_results', \n",
    "                 max_train_images=200, max_test_images=50, target_size=(256, 256)):\n",
    "        self.train_dir = train_dir\n",
    "        self.test_dir = test_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.max_train_images = max_train_images\n",
    "        self.max_test_images = max_test_images\n",
    "        self.target_size = target_size\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Create output directories\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(f\"{output_dir}/examples\", exist_ok=True)\n",
    "        os.makedirs(f\"{output_dir}/models\", exist_ok=True)\n",
    "        \n",
    "        # Get list of training and test images\n",
    "        self.train_files = self._get_image_files(self.train_dir, self.max_train_images)\n",
    "        self.test_files = self._get_image_files(self.test_dir, self.max_test_images)\n",
    "        print(f\"Found {len(self.train_files)} training images and {len(self.test_files)} test images.\")\n",
    "        \n",
    "        # Results storage\n",
    "        self.results = []\n",
    "        \n",
    "    def _get_image_files(self, directory, max_files):\n",
    "        \"\"\"Get list of image files from specified directory.\"\"\"\n",
    "        image_files = []\n",
    "        for ext in ['.tiff', '.tif', '.jpg', '.jpeg', '.png', '.bmp']:\n",
    "            image_files.extend(Path(directory).glob(f'*{ext}'))\n",
    "        return sorted(image_files)[:max_files]\n",
    "    \n",
    "    def load_image(self, path, resize=None):\n",
    "        \"\"\"Load an image and convert to RGB.\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Error: File not found at {path}\")\n",
    "                return None\n",
    "            rgb = cv2.imread(str(path))\n",
    "            if rgb is None:\n",
    "                print(f\"Error: Could not read image at {path}.\")\n",
    "                return None\n",
    "            rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "            if resize is not None:\n",
    "                rgb = cv2.resize(rgb, resize, interpolation=cv2.INTER_AREA)\n",
    "            return rgb\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def save_image_comparison(self, original, reconstructed, filename, metrics=None):\n",
    "        \"\"\"Save side-by-side comparison of original and reconstructed images.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        \n",
    "        axes[0].imshow(original)\n",
    "        axes[0].set_title('Original')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(reconstructed)\n",
    "        title = 'Reconstructed'\n",
    "        if metrics:\n",
    "            title += f'\\nPSNR: {metrics[\"psnr\"]:.2f} dB, SSIM: {metrics[\"ssim\"]:.4f}'\n",
    "        axes[1].set_title(title)\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.output_dir}/examples/{filename}', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def calculate_metrics(self, original, reconstructed):\n",
    "        \"\"\"Calculate compression metrics.\"\"\"\n",
    "        if len(original.shape) == 3:\n",
    "            original_gray = cv2.cvtColor(original, cv2.COLOR_RGB2GRAY)\n",
    "            reconstructed_gray = cv2.cvtColor(reconstructed, cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            original_gray = original\n",
    "            reconstructed_gray = reconstructed\n",
    "            \n",
    "        psnr_val = psnr(original, reconstructed, data_range=255)\n",
    "        ssim_val = ssim(original_gray, reconstructed_gray, data_range=255)\n",
    "        \n",
    "        return {'psnr': psnr_val, 'ssim': ssim_val}\n",
    "    \n",
    "    def train_autoencoder(self, images, epochs=100, batch_size=8, learning_rate=0.001):\n",
    "        \"\"\"Train autoencoder on images.\"\"\"\n",
    "        dataset = ImageDataset(images)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        input_channels = 3\n",
    "        model = SimpleAutoencoder(input_channels=input_channels, input_size=self.target_size[0]).to(self.device)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Autoencoder initialized with {total_params:,} parameters\")\n",
    "        print(f\"Training on {len(images)} images\")\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        model.train()\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        train_losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch in dataloader:\n",
    "                batch = batch.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                reconstructed = model(batch)\n",
    "                loss = criterion(reconstructed, batch)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            train_losses.append(avg_loss)\n",
    "            \n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), f'{self.output_dir}/models/best_autoencoder.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}\")\n",
    "            \n",
    "            if patience_counter >= 25:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_losses)\n",
    "        plt.title('Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MSE Loss')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig(f'{self.output_dir}/training_loss.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Training completed. Best loss: {best_loss:.6f}\")\n",
    "        model.load_state_dict(torch.load(f'{self.output_dir}/models/best_autoencoder.pth'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def compress_decompress_image(self, image, model):\n",
    "        \"\"\"Compress and decompress an image using the autoencoder.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Normalization handled by dataset\n",
    "        image_tensor = torch.from_numpy(image).float() / 255.0\n",
    "        image_tensor = image_tensor.permute(2, 0, 1).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            latent = model.encode(image_tensor)\n",
    "            reconstructed_tensor = model.decode(latent)\n",
    "        \n",
    "        reconstructed = reconstructed_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "        reconstructed = np.clip(reconstructed * 255.0, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Estimate compressed size using a simple proxy\n",
    "        latent_bytes = pickle.dumps(latent.cpu().numpy().astype(np.float32))\n",
    "        compressed_size = len(latent_bytes)\n",
    "        \n",
    "        compression_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            'reconstructed': reconstructed,\n",
    "            'compressed_size': compressed_size,\n",
    "            'compression_time': compression_time\n",
    "        }\n",
    "    \n",
    "    def analyze_results(self):\n",
    "        \"\"\"Analyze and visualize compression results.\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to analyze!\")\n",
    "            return\n",
    "            \n",
    "        df = pd.DataFrame(self.results)\n",
    "        \n",
    "        original_size = self.target_size[0] * self.target_size[1] * 3  # RGB\n",
    "        df['compression_ratio'] = (original_size * 8) / df['compressed_size']\n",
    "        df['bits_per_pixel'] = df['compressed_size'] / (self.target_size[0] * self.target_size[1])\n",
    "        \n",
    "        df.to_csv(f'{self.output_dir}/autoencoder_results.csv', index=False)\n",
    "        print(f\"Results saved to {self.output_dir}/autoencoder_results.csv\")\n",
    "        \n",
    "        self._plot_quality_analysis(df)\n",
    "        self._plot_compression_analysis(df, original_size)\n",
    "    \n",
    "    def _plot_quality_analysis(self, df):\n",
    "        \"\"\"Plot PSNR and SSIM distributions.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        sns.histplot(df['psnr'], bins=20, ax=axes[0], color='skyblue', edgecolor='black')\n",
    "        axes[0].set_xlabel('PSNR (dB)')\n",
    "        axes[0].set_title('PSNR Distribution')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        sns.histplot(df['ssim'], bins=20, ax=axes[1], color='lightgreen', edgecolor='black')\n",
    "        axes[1].set_xlabel('SSIM')\n",
    "        axes[1].set_title('SSIM Distribution')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.output_dir}/quality_analysis.png', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_compression_analysis(self, df, original_size):\n",
    "        \"\"\"Plot compression ratio and BPP vs quality.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        axes[0].scatter(df['compression_ratio'], df['psnr'], alpha=0.6, color='purple')\n",
    "        axes[0].set_xlabel('Compression Ratio')\n",
    "        axes[0].set_ylabel('PSNR (dB)')\n",
    "        axes[0].set_title('Compression Ratio vs Quality')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "        axes[1].scatter(df['bits_per_pixel'], df['psnr'], alpha=0.6, color='orange')\n",
    "        axes[1].set_xlabel('Bits per Pixel (bpp)')\n",
    "        axes[1].set_ylabel('PSNR (dB)')\n",
    "        axes[1].set_title('Rate-Distortion Curve')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.output_dir}/compression_analysis.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "def run_simple_auto_encoder_experiment():\n",
    "    \"\"\"Main function to run the standalone autoencoder experiment.\"\"\"\n",
    "    print(\"Starting Standalone Autoencoder Experiment\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    experiment = SimpleAutoencoderExperiment(\n",
    "        max_train_images=200, \n",
    "        max_test_images=50,\n",
    "        target_size=(256, 256)\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PHASE 1: LOADING TRAINING IMAGES\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    training_images = []\n",
    "    for train_file in tqdm(experiment.train_files, desc=\"Loading training images\"):\n",
    "        image = experiment.load_image(train_file, resize=experiment.target_size)\n",
    "        if image is not None:\n",
    "            training_images.append(image)\n",
    "    \n",
    "    print(f\"Successfully loaded {len(training_images)} training images\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PHASE 2: TRAINING AUTOENCODER\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if training_images:\n",
    "        autoencoder_model = experiment.train_autoencoder(training_images, epochs=100)\n",
    "        print(\"Autoencoder training completed!\")\n",
    "    else:\n",
    "        print(\"No training images available!\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PHASE 3: TESTING ON TEST IMAGES\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for i, test_file in enumerate(tqdm(experiment.test_files, desc=\"Processing test images\")):\n",
    "        image = experiment.load_image(test_file, resize=experiment.target_size)\n",
    "        if image is None:\n",
    "            continue\n",
    "        \n",
    "        result = experiment.compress_decompress_image(image, autoencoder_model)\n",
    "        \n",
    "        metrics = experiment.calculate_metrics(image, result['reconstructed'])\n",
    "        \n",
    "        experiment.results.append({\n",
    "            'image_id': i,\n",
    "            'image_name': test_file.name,\n",
    "            'compressed_size': result['compressed_size'],\n",
    "            'psnr': metrics['psnr'],\n",
    "            'ssim': metrics['ssim'],\n",
    "            'compression_time': result['compression_time']\n",
    "        })\n",
    "        \n",
    "        if i < 10:\n",
    "            experiment.save_image_comparison(\n",
    "                image, \n",
    "                result['reconstructed'], \n",
    "                f\"example_{i:03d}_{test_file.stem}.png\",\n",
    "                metrics\n",
    "            )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PHASE 4: ANALYZING RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    experiment.analyze_results()\n",
    "    \n",
    "    print(f\"\\nExperiment completed successfully!\")\n",
    "    print(f\"Results saved to {experiment.output_dir}/\")\n",
    "    print(f\"Model weights saved to {experiment.output_dir}/models/\")\n",
    "    print(f\"Example images saved to {experiment.output_dir}/examples/\")\n",
    "\n",
    "run_simple_auto_encoder_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19ba9a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Standalone Autoencoder Experiment\n",
      "==================================================\n",
      "Using device: mps\n",
      "Found 200 training images and 50 test images.\n",
      "\n",
      "==================================================\n",
      "PHASE 1: LOADING TRAINING IMAGES\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading training images: 100%|██████████| 200/200 [00:50<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 200 training images\n",
      "\n",
      "==================================================\n",
      "PHASE 2: TRAINING AUTOENCODER\n",
      "==================================================\n",
      "VAE initialized with 26,611,651 parameters\n",
      "Training on 200 images\n",
      "Epoch [5/100], Loss: 970147.442500\n",
      "Epoch [10/100], Loss: 918504.185000\n",
      "Epoch [15/100], Loss: 910506.147500\n",
      "Epoch [20/100], Loss: 901458.415000\n",
      "Epoch [25/100], Loss: 885001.495000\n",
      "Epoch [30/100], Loss: 869600.720000\n",
      "Epoch [35/100], Loss: 857716.302500\n",
      "Epoch [40/100], Loss: 849286.725000\n",
      "Epoch [45/100], Loss: 842877.770000\n",
      "Epoch [50/100], Loss: 839832.077500\n",
      "Epoch [55/100], Loss: 826528.587500\n",
      "Epoch [60/100], Loss: 822755.112500\n",
      "Epoch [65/100], Loss: 822974.800000\n",
      "Epoch [70/100], Loss: 817280.165000\n",
      "Epoch [75/100], Loss: 813347.000000\n",
      "Epoch [80/100], Loss: 810363.877500\n",
      "Epoch [85/100], Loss: 809325.207500\n",
      "Epoch [90/100], Loss: 811064.817500\n",
      "Epoch [95/100], Loss: 806017.480000\n",
      "Epoch [100/100], Loss: 803363.670000\n",
      "Training completed. Best loss: 802698.887500\n",
      "Autoencoder training completed!\n",
      "\n",
      "==================================================\n",
      "PHASE 3: TESTING ON TEST IMAGES\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test images: 100%|██████████| 50/50 [00:14<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PHASE 4: ANALYZING RESULTS\n",
      "==================================================\n",
      "Results saved to vae_results/vae_results.csv\n",
      "\n",
      "Experiment completed successfully!\n",
      "Results saved to vae_results/\n",
      "Model weights saved to vae_results/models/\n",
      "Example images saved to vae_results/examples/\n"
     ]
    }
   ],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_channels=3, input_size=256, latent_dim=128):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, 4, stride=2, padding=1), # 128x128\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1),  # 64x64\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),  # 32x32\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1), # 16x16\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # Bottleneck\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.encoder_output_size = 16 * 16 * 256\n",
    "        self.fc_mu = nn.Linear(self.encoder_output_size, self.latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.encoder_output_size, self.latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_linear = nn.Linear(self.latent_dim, self.encoder_output_size)\n",
    "        self.unflatten = nn.Unflatten(1, (256, 16, 16))\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1), # 32x32\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1), # 64x64\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),  # 128x128\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, input_channels, 4, stride=2, padding=1), # 256x256\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.flatten(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        x = self.decoder_linear(z)\n",
    "        x = self.unflatten(x)\n",
    "        return self.decoder(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# VAE Loss function\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    # Reconstruction loss (BCE or MSE)\n",
    "    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    \n",
    "    # KL-divergence loss\n",
    "    # KL_D = -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    return recon_loss + kl_div\n",
    "\n",
    "class VariationalAutoencoderExperiment:\n",
    "    \"\"\"Main class for conducting the standalone VAE compression experiment.\"\"\"\n",
    "    \n",
    "    def __init__(self, train_dir='TRAIN', test_dir='TEST', output_dir='vae_results', \n",
    "                 max_train_images=200, max_test_images=50, target_size=(256, 256)):\n",
    "        self.train_dir = train_dir\n",
    "        self.test_dir = test_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.max_train_images = max_train_images\n",
    "        self.max_test_images = max_test_images\n",
    "        self.target_size = target_size\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "        \n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        os.makedirs(f\"{output_dir}/examples\", exist_ok=True)\n",
    "        os.makedirs(f\"{output_dir}/models\", exist_ok=True)\n",
    "        \n",
    "        self.train_files = self._get_image_files(self.train_dir, self.max_train_images)\n",
    "        self.test_files = self._get_image_files(self.test_dir, self.max_test_images)\n",
    "        print(f\"Found {len(self.train_files)} training images and {len(self.test_files)} test images.\")\n",
    "        \n",
    "        self.results = []\n",
    "        \n",
    "    def _get_image_files(self, directory, max_files):\n",
    "        \"\"\"Get list of image files from specified directory.\"\"\"\n",
    "        image_files = []\n",
    "        for ext in ['.tiff', '.tif', '.jpg', '.jpeg', '.png', '.bmp']:\n",
    "            image_files.extend(Path(directory).glob(f'*{ext}'))\n",
    "        return sorted(image_files)[:max_files]\n",
    "    \n",
    "    def load_image(self, path, resize=None):\n",
    "        \"\"\"Load an image and convert to RGB.\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"Error: File not found at {path}\")\n",
    "                return None\n",
    "            rgb = cv2.imread(str(path))\n",
    "            if rgb is None:\n",
    "                print(f\"Error: Could not read image at {path}.\")\n",
    "                return None\n",
    "            rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "            if resize is not None:\n",
    "                rgb = cv2.resize(rgb, resize, interpolation=cv2.INTER_AREA)\n",
    "            return rgb\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def save_image_comparison(self, original, reconstructed, filename, metrics=None):\n",
    "        \"\"\"Save side-by-side comparison of original and reconstructed images.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        \n",
    "        axes[0].imshow(original)\n",
    "        axes[0].set_title('Original')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(reconstructed)\n",
    "        title = 'Reconstructed'\n",
    "        if metrics:\n",
    "            title += f'\\nPSNR: {metrics[\"psnr\"]:.2f} dB, SSIM: {metrics[\"ssim\"]:.4f}'\n",
    "        axes[1].set_title(title)\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.output_dir}/examples/{filename}', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def calculate_metrics(self, original, reconstructed):\n",
    "        \"\"\"Calculate compression metrics.\"\"\"\n",
    "        # Check if images are identical; if so, PSNR is infinity and SSIM is 1.0\n",
    "        if np.all(original == reconstructed):\n",
    "            psnr_val = np.inf\n",
    "            ssim_val = 1.0\n",
    "        else:\n",
    "            if len(original.shape) == 3:\n",
    "                original_gray = cv2.cvtColor(original, cv2.COLOR_RGB2GRAY)\n",
    "                reconstructed_gray = cv2.cvtColor(reconstructed, cv2.COLOR_RGB2GRAY)\n",
    "            else:\n",
    "                original_gray = original\n",
    "                reconstructed_gray = reconstructed\n",
    "                \n",
    "            psnr_val = psnr(original, reconstructed, data_range=255)\n",
    "            ssim_val = ssim(original_gray, reconstructed_gray, data_range=255)\n",
    "        \n",
    "        return {'psnr': psnr_val, 'ssim': ssim_val}\n",
    "    \n",
    "    def train_autoencoder(self, images, epochs=100, batch_size=8, learning_rate=0.001):\n",
    "        \"\"\"Train autoencoder on images.\"\"\"\n",
    "        dataset = ImageDataset(images)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        input_channels = 3\n",
    "        model = VariationalAutoencoder(input_channels=input_channels, input_size=self.target_size[0]).to(self.device)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"VAE initialized with {total_params:,} parameters\")\n",
    "        print(f\"Training on {len(images)} images\")\n",
    "        \n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        model.train()\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        train_losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch in dataloader:\n",
    "                batch = batch.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                reconstructed, mu, logvar = model(batch)\n",
    "                loss = vae_loss(reconstructed, batch, mu, logvar)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            train_losses.append(avg_loss)\n",
    "            \n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), f'{self.output_dir}/models/best_vae.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.6f}\")\n",
    "            \n",
    "            if patience_counter >= 25:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_losses)\n",
    "        plt.title('Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('VAE Loss')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig(f'{self.output_dir}/training_loss.png', dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Training completed. Best loss: {best_loss:.6f}\")\n",
    "        model.load_state_dict(torch.load(f'{self.output_dir}/models/best_vae.pth'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def compress_decompress_image(self, image, model):\n",
    "        \"\"\"Compress and decompress an image using the autoencoder.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Normalization handled by dataset\n",
    "        image_tensor = torch.from_numpy(image).float() / 255.0\n",
    "        image_tensor = image_tensor.permute(2, 0, 1).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Get the mean vector as the latent representation for compression\n",
    "            mu, _ = model.encode(image_tensor)\n",
    "            reconstructed_tensor = model.decode(mu)\n",
    "        \n",
    "        reconstructed = reconstructed_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "        reconstructed = np.clip(reconstructed * 255.0, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Estimate compressed size using a simple proxy\n",
    "        latent_bytes = pickle.dumps(mu.cpu().numpy().astype(np.float32))\n",
    "        compressed_size = len(latent_bytes)\n",
    "        \n",
    "        compression_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            'reconstructed': reconstructed,\n",
    "            'compressed_size': compressed_size,\n",
    "            'compression_time': compression_time\n",
    "        }\n",
    "    \n",
    "    def analyze_results(self):\n",
    "        \"\"\"Analyze and visualize compression results.\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to analyze!\")\n",
    "            return\n",
    "            \n",
    "        df = pd.DataFrame(self.results)\n",
    "        \n",
    "        original_size = self.target_size[0] * self.target_size[1] * 3 # RGB\n",
    "        df['compression_ratio'] = (original_size * 8) / df['compressed_size']\n",
    "        df['bits_per_pixel'] = df['compressed_size'] / (self.target_size[0] * self.target_size[1])\n",
    "        \n",
    "        df.to_csv(f'{self.output_dir}/vae_results.csv', index=False)\n",
    "        print(f\"Results saved to {self.output_dir}/vae_results.csv\")\n",
    "        \n",
    "        self._plot_quality_analysis(df)\n",
    "        self._plot_compression_analysis(df, original_size)\n",
    "    \n",
    "    def _plot_quality_analysis(self, df):\n",
    "        \"\"\"Plot PSNR and SSIM distributions.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        sns.histplot(df['psnr'], bins=20, ax=axes[0], color='skyblue', edgecolor='black')\n",
    "        axes[0].set_xlabel('PSNR (dB)')\n",
    "        axes[0].set_title('PSNR Distribution')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        sns.histplot(df['ssim'], bins=20, ax=axes[1], color='lightgreen', edgecolor='black')\n",
    "        axes[1].set_xlabel('SSIM')\n",
    "        axes[1].set_title('SSIM Distribution')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.output_dir}/quality_analysis.png', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def _plot_compression_analysis(self, df, original_size):\n",
    "        \"\"\"Plot compression ratio and BPP vs quality.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        axes[0].scatter(df['compression_ratio'], df['psnr'], alpha=0.6, color='purple')\n",
    "        axes[0].set_xlabel('Compression Ratio')\n",
    "        axes[0].set_ylabel('PSNR (dB)')\n",
    "        axes[0].set_title('Compression Ratio vs Quality')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "        axes[1].scatter(df['bits_per_pixel'], df['psnr'], alpha=0.6, color='orange')\n",
    "        axes[1].set_xlabel('Bits per Pixel (bpp)')\n",
    "        axes[1].set_ylabel('PSNR (dB)')\n",
    "        axes[1].set_title('Rate-Distortion Curve')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.output_dir}/compression_analysis.png', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "def run_vae_experiment():\n",
    "    \"\"\"Main function to run the standalone autoencoder experiment.\"\"\"\n",
    "    print(\"Starting Standalone Autoencoder Experiment\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    experiment = VariationalAutoencoderExperiment(\n",
    "        max_train_images=200, \n",
    "        max_test_images=50,\n",
    "        target_size=(256, 256)\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PHASE 1: LOADING TRAINING IMAGES\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    training_images = []\n",
    "    for train_file in tqdm(experiment.train_files, desc=\"Loading training images\"):\n",
    "        image = experiment.load_image(train_file, resize=experiment.target_size)\n",
    "        if image is not None:\n",
    "            training_images.append(image)\n",
    "    \n",
    "    print(f\"Successfully loaded {len(training_images)} training images\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PHASE 2: TRAINING AUTOENCODER\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if training_images:\n",
    "        autoencoder_model = experiment.train_autoencoder(training_images, epochs=100)\n",
    "        print(\"Autoencoder training completed!\")\n",
    "    else:\n",
    "        print(\"No training images available!\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PHASE 3: TESTING ON TEST IMAGES\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for i, test_file in enumerate(tqdm(experiment.test_files, desc=\"Processing test images\")):\n",
    "        image = experiment.load_image(test_file, resize=experiment.target_size)\n",
    "        if image is None:\n",
    "            continue\n",
    "        \n",
    "        result = experiment.compress_decompress_image(image, autoencoder_model)\n",
    "        \n",
    "        metrics = experiment.calculate_metrics(image, result['reconstructed'])\n",
    "        \n",
    "        experiment.results.append({\n",
    "            'image_id': i,\n",
    "            'image_name': test_file.name,\n",
    "            'compressed_size': result['compressed_size'],\n",
    "            'psnr': metrics['psnr'],\n",
    "            'ssim': metrics['ssim'],\n",
    "            'compression_time': result['compression_time']\n",
    "        })\n",
    "        \n",
    "        if i < 10:\n",
    "            experiment.save_image_comparison(\n",
    "                image, \n",
    "                result['reconstructed'], \n",
    "                f\"example_{i:03d}_{test_file.stem}.png\",\n",
    "                metrics\n",
    "            )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PHASE 4: ANALYZING RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    experiment.analyze_results()\n",
    "    \n",
    "    print(f\"\\nExperiment completed successfully!\")\n",
    "    print(f\"Results saved to {experiment.output_dir}/\")\n",
    "    print(f\"Model weights saved to {experiment.output_dir}/models/\")\n",
    "    print(f\"Example images saved to {experiment.output_dir}/examples/\")\n",
    "\n",
    "run_vae_experiment()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2882fa52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating hybrid compression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 54/54 [00:14<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete. Generating plots and examples.\n"
     ]
    }
   ],
   "source": [
    "def save_comparison(original, reconstructed, filename, metrics, output_dir):\n",
    "    \"\"\"Save side-by-side comparison.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axes[0].imshow(original)\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(reconstructed)\n",
    "    axes[1].set_title(f'Hybrid Reconstruction\\nPSNR: {metrics[\"psnr\"]:.2f} dB, SSIM: {metrics[\"ssim\"]:.4f}')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, filename), dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_autoencoder_hybrid_approach(\n",
    "    model_path='autoencoder_results/models/best_autoencoder.pth', \n",
    "    images_dir='TEST', \n",
    "    output_dir='auto_encoder_hybrid_results',\n",
    "    target_size=(256, 256),\n",
    "    num_examples=10,\n",
    "    quantization_bits=4\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates a hybrid image compression approach using an autoencoder and residual compression.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = SimpleAutoencoder(input_channels=3, input_size=target_size[0]).to(device)\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: Model file not found at {model_path}\")\n",
    "        return\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'examples'), exist_ok=True)\n",
    "    \n",
    "    image_files = [f for f in Path(images_dir).glob('*') if f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']]\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_dir}\")\n",
    "        return\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    print(\"Evaluating hybrid compression...\")\n",
    "    \n",
    "    # Use torch.no_grad() for the entire evaluation block\n",
    "    with torch.no_grad():\n",
    "        for img_path in tqdm(image_files, desc=\"Processing images\"):\n",
    "            original_img = load_image(img_path, resize=target_size)\n",
    "            if original_img is None:\n",
    "                continue\n",
    "\n",
    "            # Step 1: Get autoencoder reconstruction\n",
    "            img_tensor = torch.from_numpy(original_img).float() / 255.0\n",
    "            img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "            reconstructed_tensor = model(img_tensor)\n",
    "            \n",
    "            # Detach and convert to numpy\n",
    "            reconstructed_img = (reconstructed_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255.0).astype(np.uint8)\n",
    "\n",
    "            # Step 2: Diff and quantize residuals\n",
    "            residuals = original_img.astype(np.int16) - reconstructed_img.astype(np.int16)\n",
    "            \n",
    "            # Quantize residuals to a limited number of levels\n",
    "            scale = 255.0 / (2**quantization_bits - 1)\n",
    "            quantized_residuals = np.round(residuals / scale).astype(np.int8)\n",
    "\n",
    "            # Step 3: Compress with zstd\n",
    "            cctx = zstd.ZstdCompressor()\n",
    "            compressed_residuals = cctx.compress(quantized_residuals.tobytes())\n",
    "            compressed_size_residuals = len(compressed_residuals)\n",
    "\n",
    "            # Step 4: Bitpack (not implemented in a simple way here, but size is accounted for)\n",
    "\n",
    "            # Step 5: Total compressed size\n",
    "            latent_tensor = model.encode(img_tensor)\n",
    "            latent_size = len(latent_tensor.cpu().numpy().tobytes())\n",
    "            total_compressed_size = latent_size + compressed_size_residuals\n",
    "            \n",
    "            # Step 6: Reconstruct the image from the hybrid data\n",
    "            # Decompress residuals\n",
    "            dctx = zstd.ZstdDecompressor()\n",
    "            decompressed_residuals_bytes = dctx.decompress(compressed_residuals)\n",
    "            decompressed_residuals = np.frombuffer(decompressed_residuals_bytes, dtype=np.int8).reshape(target_size[0], target_size[1], 3)\n",
    "            \n",
    "            # De-quantize\n",
    "            final_residuals = (decompressed_residuals.astype(np.int16) * scale).astype(np.int16)\n",
    "            \n",
    "            # Add back to the autoencoder reconstruction\n",
    "            final_reconstruction = np.clip(reconstructed_img.astype(np.int16) + final_residuals, 0, 255).astype(np.uint8)\n",
    "\n",
    "            # Step 7: Recompute metrics\n",
    "            metrics = calculate_metrics(original_img, final_reconstruction)\n",
    "            \n",
    "            original_bytes = target_size[0] * target_size[1] * 3\n",
    "            compression_ratio = original_bytes / total_compressed_size\n",
    "            bits_per_pixel = (total_compressed_size * 8) / (target_size[0] * target_size[1])\n",
    "\n",
    "            results.append({\n",
    "                'image_name': img_path.name,\n",
    "                'psnr': metrics['psnr'],\n",
    "                'ssim': metrics['ssim'],\n",
    "                'compression_ratio': compression_ratio,\n",
    "                'bits_per_pixel': bits_per_pixel,\n",
    "                'original_size': original_bytes,\n",
    "                'sum_of_latent_and_residual': total_compressed_size,\n",
    "                'latent_size': latent_size,\n",
    "                'residual_size': compressed_size_residuals\n",
    "            })\n",
    "\n",
    "    print(\"Analysis complete. Generating plots and examples.\")\n",
    "    \n",
    "    # Analyze and plot results\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(output_dir, 'hybrid_results.csv'), index=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    sns.histplot(df['psnr'], bins=20, ax=axes[0])\n",
    "    axes[0].set_title('Hybrid PSNR Distribution')\n",
    "    sns.histplot(df['compression_ratio'], bins=20, ax=axes[1])\n",
    "    axes[1].set_title('Hybrid Compression Ratio Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'hybrid_metrics.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Save a few example images\n",
    "    with torch.no_grad():\n",
    "        for i in range(min(num_examples, len(image_files))):\n",
    "            img_path = image_files[i]\n",
    "            original_img = load_image(img_path, resize=target_size)\n",
    "            \n",
    "            img_tensor = torch.from_numpy(original_img).float() / 255.0\n",
    "            img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "            reconstructed_tensor = model(img_tensor)\n",
    "            \n",
    "            reconstructed_img = (reconstructed_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255.0).astype(np.uint8)\n",
    "\n",
    "            residuals = original_img.astype(np.int16) - reconstructed_img.astype(np.int16)\n",
    "            scale = 255.0 / (2**quantization_bits - 1)\n",
    "            quantized_residuals = np.round(residuals / scale).astype(np.int8)\n",
    "\n",
    "            cctx = zstd.ZstdCompressor()\n",
    "            compressed_residuals = cctx.compress(quantized_residuals.tobytes())\n",
    "            \n",
    "            dctx = zstd.ZstdDecompressor()\n",
    "            decompressed_residuals_bytes = dctx.decompress(compressed_residuals)\n",
    "            decompressed_residuals = np.frombuffer(decompressed_residuals_bytes, dtype=np.int8).reshape(target_size[0], target_size[1], 3)\n",
    "            final_residuals = (decompressed_residuals.astype(np.int16) * scale).astype(np.int16)\n",
    "            final_reconstruction = np.clip(reconstructed_img.astype(np.int16) + final_residuals, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            metrics = calculate_metrics(original_img, final_reconstruction)\n",
    "            save_comparison(original_img, final_reconstruction, f'hybrid_example_{i:02d}.png', metrics, os.path.join(output_dir, 'examples'))\n",
    "\n",
    "evaluate_autoencoder_hybrid_approach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21d8bf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Evaluating hybrid VAE compression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 54/54 [00:14<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete. Generating plots and examples.\n"
     ]
    }
   ],
   "source": [
    "def evaluate_vae_hybrid_approach(\n",
    "    model_path='vae_results/models/best_vae.pth',\n",
    "    images_dir='TEST',\n",
    "    output_dir='vae_hybrid_results',\n",
    "    target_size=(256, 256),\n",
    "    num_examples=10,\n",
    "    quantization_bits=4\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates a hybrid image compression approach using a VAE and residual compression.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    model = VariationalAutoencoder(input_channels=3, input_size=target_size[0]).to(device)\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: Model file not found at {model_path}\")\n",
    "        return\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully.\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'examples'), exist_ok=True)\n",
    "    \n",
    "    image_files = [f for f in Path(images_dir).glob('*') if f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']]\n",
    "    if not image_files:\n",
    "        print(f\"No images found in {images_dir}\")\n",
    "        return\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    print(\"Evaluating hybrid VAE compression...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img_path in tqdm(image_files, desc=\"Processing images\"):\n",
    "            original_img = load_image(img_path, resize=target_size)\n",
    "            if original_img is None:\n",
    "                continue\n",
    "\n",
    "            # Step 1: Get VAE reconstruction\n",
    "            img_tensor = torch.from_numpy(original_img).float() / 255.0\n",
    "            img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "            \n",
    "            # The VAE forward pass returns a tuple\n",
    "            reconstructed_tensor, mu, logvar = model(img_tensor)\n",
    "            \n",
    "            reconstructed_img = (reconstructed_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255.0).astype(np.uint8)\n",
    "\n",
    "            # Step 2: Diff and quantize residuals\n",
    "            residuals = original_img.astype(np.int16) - reconstructed_img.astype(np.int16)\n",
    "            \n",
    "            scale = 255.0 / (2**quantization_bits - 1)\n",
    "            quantized_residuals = np.round(residuals / scale).astype(np.int8)\n",
    "\n",
    "            # Step 3: Compress with zstd\n",
    "            cctx = zstd.ZstdCompressor()\n",
    "            compressed_residuals = cctx.compress(quantized_residuals.tobytes())\n",
    "            compressed_size_residuals = len(compressed_residuals)\n",
    "\n",
    "            # Step 4: Total compressed size\n",
    "            # Use the size of the 'mu' vector as the latent size\n",
    "            latent_size = mu.numel() * 4  # Assuming float32 data type (4 bytes per element)\n",
    "            total_compressed_size = latent_size + compressed_size_residuals\n",
    "            \n",
    "            # Step 5: Reconstruct the image from the hybrid data\n",
    "            dctx = zstd.ZstdDecompressor()\n",
    "            decompressed_residuals_bytes = dctx.decompress(compressed_residuals)\n",
    "            decompressed_residuals = np.frombuffer(decompressed_residuals_bytes, dtype=np.int8).reshape(target_size[0], target_size[1], 3)\n",
    "            \n",
    "            # De-quantize\n",
    "            final_residuals = (decompressed_residuals.astype(np.int16) * scale).astype(np.int16)\n",
    "            \n",
    "            final_reconstruction = np.clip(reconstructed_img.astype(np.int16) + final_residuals, 0, 255).astype(np.uint8)\n",
    "\n",
    "            # Step 6: Recompute metrics\n",
    "            metrics = calculate_metrics(original_img, final_reconstruction)\n",
    "            \n",
    "            original_bytes = target_size[0] * target_size[1] * 3\n",
    "            compression_ratio = original_bytes / total_compressed_size\n",
    "            bits_per_pixel = (total_compressed_size * 8) / (target_size[0] * target_size[1])\n",
    "\n",
    "            results.append({\n",
    "                'image_name': img_path.name,\n",
    "                'psnr': metrics['psnr'],\n",
    "                'ssim': metrics['ssim'],\n",
    "                'compression_ratio': compression_ratio,\n",
    "                'bits_per_pixel': bits_per_pixel,\n",
    "                'original_size': original_bytes,\n",
    "                'sum_of_latent_and_residual': total_compressed_size,\n",
    "                'latent_size': latent_size,\n",
    "                'residual_size': compressed_size_residuals\n",
    "            })\n",
    "\n",
    "    print(\"Analysis complete. Generating plots and examples.\")\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(output_dir, 'hybrid_results.csv'), index=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    sns.histplot(df['psnr'], bins=20, ax=axes[0])\n",
    "    axes[0].set_title('Hybrid PSNR Distribution')\n",
    "    sns.histplot(df['compression_ratio'], bins=20, ax=axes[1])\n",
    "    axes[1].set_title('Hybrid Compression Ratio Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'hybrid_metrics.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(min(num_examples, len(image_files))):\n",
    "            img_path = image_files[i]\n",
    "            original_img = load_image(img_path, resize=target_size)\n",
    "            \n",
    "            img_tensor = torch.from_numpy(original_img).float() / 255.0\n",
    "            img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Unpack the VAE output\n",
    "            reconstructed_tensor, mu, logvar = model(img_tensor)\n",
    "            \n",
    "            reconstructed_img = (reconstructed_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255.0).astype(np.uint8)\n",
    "\n",
    "            residuals = original_img.astype(np.int16) - reconstructed_img.astype(np.int16)\n",
    "            scale = 255.0 / (2**quantization_bits - 1)\n",
    "            quantized_residuals = np.round(residuals / scale).astype(np.int8)\n",
    "\n",
    "            cctx = zstd.ZstdCompressor()\n",
    "            compressed_residuals = cctx.compress(quantized_residuals.tobytes())\n",
    "            \n",
    "            dctx = zstd.ZstdDecompressor()\n",
    "            decompressed_residuals_bytes = dctx.decompress(compressed_residuals)\n",
    "            decompressed_residuals = np.frombuffer(decompressed_residuals_bytes, dtype=np.int8).reshape(target_size[0], target_size[1], 3)\n",
    "            final_residuals = (decompressed_residuals.astype(np.int16) * scale).astype(np.int16)\n",
    "            final_reconstruction = np.clip(reconstructed_img.astype(np.int16) + final_residuals, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            metrics = calculate_metrics(original_img, final_reconstruction)\n",
    "            save_comparison(original_img, final_reconstruction, f'hybrid_example_{i:02d}.png', metrics, os.path.join(output_dir, 'examples'))\n",
    "\n",
    "evaluate_vae_hybrid_approach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07905cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box plots saved to boxplots/auto_encoder_hybrid_boxplots.png\n",
      "Box plots saved to boxplots/vae_hybrid_boxplots.png\n",
      "Box plots saved to boxplots/raw_jpeg_boxplots.png\n"
     ]
    }
   ],
   "source": [
    "def create_boxplots_from_file(csv_filepath, output_filename='boxplots.png'):\n",
    "    \"\"\"\n",
    "    Reads data from a CSV file, creates side-by-side boxplots for PSNR, SSIM,\n",
    "    and Compression Ratio, and saves the plot to a file.\n",
    "\n",
    "    Args:\n",
    "        csv_filepath (str): The path to the CSV file.\n",
    "        output_filename (str): The name of the file to save the plot to.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(csv_filepath):\n",
    "            print(f\"Error: The file '{csv_filepath}' does not exist.\")\n",
    "            return\n",
    "        \n",
    "        df = pd.read_csv(csv_filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file '{csv_filepath}': {e}\")\n",
    "        return\n",
    "\n",
    "    # Check for the required columns\n",
    "    required_columns = ['psnr', 'ssim', 'compression_ratio']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        print(\"Error: The CSV data must contain 'psnr', 'ssim', and 'compression_ratio' columns.\")\n",
    "        return\n",
    "    \n",
    "    # Create a figure and a set of subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle('Distribution of Key Performance Metrics', fontsize=16)\n",
    "\n",
    "    # Boxplot for PSNR\n",
    "    axes[0].boxplot(df['psnr'], patch_artist=True, boxprops=dict(facecolor='lightblue'))\n",
    "    axes[0].set_title('PSNR Distribution')\n",
    "    axes[0].set_ylabel('PSNR (dB)')\n",
    "    axes[0].set_xticks([1], [''])\n",
    "\n",
    "    # Boxplot for SSIM\n",
    "    axes[1].boxplot(df['ssim'], patch_artist=True, boxprops=dict(facecolor='lightgreen'))\n",
    "    axes[1].set_title('SSIM Distribution')\n",
    "    axes[1].set_ylabel('SSIM Value')\n",
    "    axes[1].set_xticks([1], [''])\n",
    "\n",
    "    # Boxplot for Compression Ratio\n",
    "    axes[2].boxplot(df['compression_ratio'], patch_artist=True, boxprops=dict(facecolor='lightcoral'))\n",
    "    axes[2].set_title('Compression Ratio Distribution')\n",
    "    axes[2].set_ylabel('Ratio')\n",
    "    axes[2].set_xticks([1], [''])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.savefig(output_filename)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Box plots saved to {output_filename}\")\n",
    "\n",
    "os.makedirs('boxplots', exist_ok=True)\n",
    "create_boxplots_from_file('auto_encoder_hybrid_results/hybrid_results.csv', 'boxplots/auto_encoder_hybrid_boxplots.png')\n",
    "create_boxplots_from_file('vae_hybrid_results/hybrid_results.csv', 'boxplots/vae_hybrid_boxplots.png')\n",
    "create_boxplots_from_file('raw_jpeg_results/raw_jpeg_results.csv', 'boxplots/raw_jpeg_boxplots.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
